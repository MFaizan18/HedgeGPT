{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9518c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Mohammed Faizan\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6203445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import bspline.splinelab as splinelab\n",
    "from scipy.interpolate import splev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "452b485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_option_parameters():\n",
    "    \"\"\"\n",
    "    Prompt for option inputs, with:\n",
    "      – μ, σ, r: accept decimal (0.05) or percent (5 → 0.05)\n",
    "      – S₀, K: absolute values\n",
    "      – M: enter in years (fractional for months/weeks), with trading-day feedback\n",
    "    \"\"\"\n",
    "\n",
    "    def get_pct_or_decimal(prompt, name, lower=0.0, upper=1.0):\n",
    "        while True:\n",
    "            try:\n",
    "                raw = float(input(prompt))\n",
    "            except ValueError:\n",
    "                print(f\"  → Please enter a valid number for {name}.\")\n",
    "                continue\n",
    "            # treat >1 as percent\n",
    "            if raw > 1:\n",
    "                print(f\"  ↳ You entered {raw:.2f}. Interpreting as {raw:.2f}% → {raw/100:.4f}.\")\n",
    "                raw /= 100\n",
    "            if not (lower <= raw <= upper):\n",
    "                print(f\"  → {name} should be between {lower:.2f} and {upper:.2f}.\")\n",
    "                continue\n",
    "            return raw\n",
    "\n",
    "    def get_absolute(prompt, name, lower=1e-6, upper=1e6):\n",
    "        while True:\n",
    "            try:\n",
    "                val = float(input(prompt))\n",
    "            except ValueError:\n",
    "                print(f\"  → Please enter a valid number for {name}.\")\n",
    "                continue\n",
    "            if not (lower <= val <= upper):\n",
    "                print(f\"  → {name} should be between {lower} and {upper}.\")\n",
    "                continue\n",
    "            return val\n",
    "\n",
    "    def get_maturity(prompt):\n",
    "        print(\"Examples:\")\n",
    "        print(\"  • For 3 months (~63 trading days), enter 0.25\")\n",
    "        print(\"  • For 6 weeks (~31 trading days), enter 0.115\")\n",
    "        print(\"  • For 9 months (~189 trading days), enter 0.75\")\n",
    "        print(\"  • For 1.5 years (~378 trading days), enter 1.5\")\n",
    "        print(\"  • For 2 years (~504 trading days), enter 2\\n\")\n",
    "        while True:\n",
    "            try:\n",
    "                val = float(input(prompt))\n",
    "            except ValueError:\n",
    "                print(\"  → Please enter a valid decimal or integer for M.\")\n",
    "                continue\n",
    "            if not (1e-4 <= val <= 50):\n",
    "                print(\"  → M should be between 0.0001 and 50 (years).\")\n",
    "                continue\n",
    "\n",
    "            # feedback in calendar months and trading days\n",
    "            months = val * 12\n",
    "            trading_days = val * 252\n",
    "            if val < 1:\n",
    "                print(f\"  ↳ You selected {val:.4f} years ≈ \"\n",
    "                      f\"{months:.1f} months (~{trading_days:.0f} trading days).\")\n",
    "            else:\n",
    "                yrs = int(val)\n",
    "                rem_months = (val - yrs) * 12\n",
    "                print(f\"  ↳ You selected {yrs} year(s) + {rem_months:.1f} months \"\n",
    "                      f\"(~{trading_days:.0f} trading days).\")\n",
    "            return val\n",
    "\n",
    "\n",
    "    S0 = get_absolute(\"1) Initial stock price S₀ (e.g. 50, 100): \", \"S₀\")\n",
    "    mu = get_pct_or_decimal(\"2) Expected drift μ (e.g. 0.05 or 5): \", \"μ\", -1.0, 1.0)\n",
    "    sigma = get_pct_or_decimal(\"3) Volatility σ (e.g. 0.15 or 15): \", \"σ\", 1e-4, 5.0)\n",
    "    r = get_pct_or_decimal(\"4) Risk-free rate r (e.g. 0.03 or 3): \", \"r\", 0.0, 1.0)\n",
    "    K = get_absolute(\"5) Strike price K (e.g. 80, 100): \", \"K\")\n",
    "    M = get_maturity(\"6) Time to maturity M in years (e.g. see examples above): \")\n",
    "   \n",
    "\n",
    "    while True:\n",
    "        option_type = input(\"7) Option type: 'call' or 'put': \").strip().lower()\n",
    "        if option_type in (\"call\", \"put\"):\n",
    "            break\n",
    "        print(\"  → Please enter either 'call' or 'put'.\")\n",
    "\n",
    "    # 11) print header for DP pricing\n",
    "    \n",
    "\n",
    "    print(f\"  S₀ = {S0}, μ = {mu}, σ = {sigma}, r = {r}, \"\n",
    "          f\"K = {K}, M = {M} years, Type = {option_type.upper()}\\n\")\n",
    "\n",
    "    return S0, mu, sigma, r, K, M, option_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "774e5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_payoff(ST, K, option_type='call'):\n",
    "    if option_type=='call':\n",
    "        return max(ST-K, 0)\n",
    "    else:\n",
    "        return max(K-ST, 0)\n",
    "\n",
    "def function_A_vec(t, delta_S_hat, data_mat, reg_param):\n",
    "    phi_t = data_mat[t,:,:]\n",
    "    dS_hat_t = delta_S_hat.iloc[:,t].astype(float)\n",
    "    dS_hat_sq = dS_hat_t**2\n",
    "    A_mat = np.dot(phi_t.T, dS_hat_sq.values[:,None]*phi_t)\n",
    "    A_mat += reg_param * np.eye(phi_t.shape[1])\n",
    "    return A_mat\n",
    "\n",
    "def function_B_vec(t, Pi_hat, delta_S_hat, delta_S, data_mat, gamma, risk_lambda):\n",
    "    phi_t = data_mat[t,:,:]\n",
    "    pi_next = Pi_hat.iloc[:,t+1].values.astype(float)\n",
    "    dS_hat_t = delta_S_hat.iloc[:,t].values.astype(float)\n",
    "    delta_S_t = delta_S.iloc[:,t].values.astype(float)\n",
    "    penalty_term = (1.0/(2*gamma*risk_lambda))*delta_S_t\n",
    "    term = pi_next*dS_hat_t + 0 # made 'penalty_term' Zero for pure hedge\n",
    "    return np.dot(phi_t.T, term)\n",
    "\n",
    "def function_C_vec(t, data_mat, reg_param):\n",
    "    phi_t = data_mat[t,:,:]\n",
    "    C_mat = np.dot(phi_t.T, phi_t)\n",
    "    C_mat += reg_param * np.eye(phi_t.shape[1])\n",
    "    return C_mat\n",
    "\n",
    "def function_D_vec(t, Q, R, data_mat, gamma):\n",
    "    phi_t = data_mat[t,:,:]\n",
    "    term = (R.iloc[:,t] + gamma*Q.iloc[:,t+1]).values.astype(float)\n",
    "    return np.dot(phi_t.T, term)\n",
    "\n",
    "def function_S_vec(t, S_t_mat, reg_param):\n",
    "    S_t = np.array(S_t_mat[:, :, t], dtype=np.float64)  # ensure float64\n",
    "    num_Qbasis = S_t.shape[0]\n",
    "    S_mat_reg = S_t + reg_param * np.eye(num_Qbasis)\n",
    "    return S_mat_reg\n",
    "  \n",
    "def function_M_vec(t, Q_star, R, Psi_mat_t, gamma):\n",
    "    R_t = R.iloc[:, t].values  # Shape (N_MC,)\n",
    "    Q_next = Q_star.loc[:, t + 1].values  # Shape (N_MC,)\n",
    "    targets = R_t + gamma * Q_next  # Shape (N_MC,)\n",
    "    Psi_np = Psi_mat_t  # Shape (num_Qbasis, N_MC)\n",
    "    M_t = np.dot(Psi_np, targets)  # Shape (num_Qbasis,)\n",
    "    return M_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "574809d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 0) prompt\n",
    "    S0, mu, sigma, r, K, M, option_type = prompt_option_parameters()\n",
    "\n",
    "    # 1) parameters\n",
    "    risk_lambda = 0.001 # Risk-aversion parameter: adjust based on desired risk sensitivity\n",
    "    N_MC        = 20000\n",
    "    T           = 6\n",
    "    delta_t     = M / T\n",
    "    gamma       = np.exp(-r*delta_t)\n",
    "    reg_param   = 1e-3\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # 2) simulate S\n",
    "    S = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
    "    S.loc[:,0] = S0\n",
    "    RN = pd.DataFrame(np.random.randn(N_MC,T), index=range(1, N_MC+1), columns=range(1, T+1))\n",
    "    for t in range(1, T+1):\n",
    "      S.loc[:,t] = S.loc[:,t-1] * np.exp((mu - 1/2 * sigma**2) * delta_t + sigma * np.sqrt(delta_t) * RN.loc[:,t])\n",
    "\n",
    "    # 3) delta's\n",
    "    delta_S = S.loc[:,1:T].values - np.exp(r * delta_t) * S.loc[:,0:T-1]\n",
    "    delta_S_hat = delta_S.apply(lambda x: x - np.mean(x), axis=0)\n",
    "\n",
    "    # 4) state X\n",
    "    X = - (mu - 1/2 * sigma**2) * np.arange(T+1) * delta_t + np.log(S.astype(float))\n",
    "\n",
    "    # 5) spline & basis\n",
    "    X_min = np.min(np.min(X))\n",
    "    X_max = np.max(np.max(X))\n",
    "    p = 4\n",
    "    ncolloc = 12\n",
    "    tau = np.linspace(X_min, X_max, ncolloc)\n",
    "    k = splinelab.aptknt(tau, p)\n",
    "    degree = p - 1\n",
    "    n_basis = len(k) - p\n",
    "    num_t_steps = T + 1\n",
    "    N_MC = X.shape[0]\n",
    "    flat_x = X.values.flatten(order='F')\n",
    "    flat_basis = np.zeros((flat_x.size, n_basis))\n",
    "    for j in range(n_basis):\n",
    "      coeffs = np.zeros(n_basis)\n",
    "      coeffs[j] = 1.0\n",
    "      flat_basis[:, j] = splev(flat_x, (k, coeffs, degree))\n",
    "    data_mat_t = flat_basis.reshape((num_t_steps, N_MC, n_basis))\n",
    "\n",
    "    # 6) replicate Pi, Pi_hat, a\n",
    "    Pi = pd.DataFrame(index=range(1, N_MC+1), columns=range(T+1))\n",
    "    Pi.iloc[:, -1] = S.iloc[:, -1].apply(lambda x: terminal_payoff(x, K, option_type))\n",
    "    Pi_hat = pd.DataFrame(index=range(1, N_MC+1), columns=range(T+1))\n",
    "    Pi_hat.iloc[:, -1] = Pi.iloc[:, -1] - np.mean(Pi.iloc[:, -1])\n",
    "    a = pd.DataFrame(index=range(1, N_MC+1), columns=range(T+1))\n",
    "    a.iloc[:, -1] = 0\n",
    "\n",
    "\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        A_mat = function_A_vec(t, delta_S_hat, data_mat_t, reg_param)\n",
    "        B_vec = function_B_vec(t, Pi_hat, delta_S_hat, delta_S, data_mat_t, gamma, risk_lambda)\n",
    "        phi = np.dot(np.linalg.inv(A_mat), B_vec)\n",
    "        a.iloc[:, t] = np.dot(data_mat_t[t, :, :], phi)\n",
    "        Pi.iloc[:, t] = gamma * (Pi.iloc[:, t+1] - a.iloc[:, t] * delta_S.iloc[:, t])\n",
    "        Pi_hat.iloc[:, t] = Pi.iloc[:, t] - Pi.iloc[:, t].mean()\n",
    "\n",
    "    # 7) rewards\n",
    "    R = pd.DataFrame(np.nan, index=range(1, N_MC+1), columns=range(T+1), dtype=np.float64)\n",
    "    R.iloc[:,-1] = -risk_lambda * np.var(Pi.iloc[:,-1].values.astype(float))\n",
    "\n",
    "    for t in range(T):\n",
    "        R.loc[:, t] = (\n",
    "            gamma * a.loc[:, t].values.astype(float) * delta_S.loc[:, t].values.astype(float)\n",
    "            - risk_lambda * np.var(Pi.loc[:, t].values.astype(float))\n",
    "        )\n",
    "\n",
    "    # 8) Q‐learning\n",
    "    Q = pd.DataFrame(np.nan, index=range(1, N_MC+1), columns=range(T+1), dtype=np.float64)\n",
    "    Q.iloc[:,-1] = (-Pi.iloc[:,-1] - risk_lambda * np.var(Pi.iloc[:,-1])).astype(float)\n",
    "\n",
    "\n",
    "    for t in range(T-1, -1, -1):\n",
    "        C_mat = function_C_vec(t,data_mat_t,reg_param)\n",
    "        D_vec = function_D_vec(t, Q,R,data_mat_t,gamma)\n",
    "        omega = np.dot(np.linalg.inv(C_mat), D_vec)\n",
    "        Q.loc[:,t] = np.dot(data_mat_t[t,:,:], omega)\n",
    "\n",
    "    on_policy_price = abs(Q.loc[:,0].mean())\n",
    "\n",
    "    print(\"\\n🚀 Running HedgeGPT Option Pricing Engine\")\n",
    "    print(f\"  • Risk-aversion (λ) = {risk_lambda:.3f}\")\n",
    "    print(\"  • Methods: On-Policy DP and Off-Policy RL\")\n",
    "    print(\"  • Benchmark: Black–Scholes Formula\\n\")\n",
    "\n",
    "    # 9) print on‑policy results\n",
    "    print(f\"✅ On-Policy Price       = {on_policy_price:.2f}\")\n",
    "  \n",
    "    # 10) off‑policy initialization\n",
    "    eta = 0.5\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Save on‑policy optimal actions\n",
    "    a_dp = a.copy()\n",
    "\n",
    "    # Initialize off-policy actions (a_op)\n",
    "    a_op = pd.DataFrame(0.0, index=range(1, N_MC + 1), columns=range(T + 1), dtype=float)\n",
    "    a_op.iloc[:, -1] = 0.0  # No action at terminal time\n",
    "\n",
    "    # Initialize off-policy portfolios\n",
    "    Pi_op = pd.DataFrame(0.0, index=range(1, N_MC + 1), columns=range(T + 1), dtype=float)\n",
    "    Pi_op.iloc[:, -1] = S.iloc[:, -1].apply(lambda x: float(terminal_payoff(x, K, option_type)))\n",
    "\n",
    "    # Initialize rewards\n",
    "    R_op = pd.DataFrame(0.0, index=range(1, N_MC + 1), columns=range(T + 1), dtype=float)\n",
    "    R_op.iloc[:, -1] = -risk_lambda * np.var(Pi_op.iloc[:, -1])\n",
    "\n",
    "    # 11) Backward simulate off‑policy\n",
    "    for t in range(T-1, -1, -1):\n",
    "        a_star_t = a_dp.iloc[:, t]\n",
    "        noise = np.random.uniform(1-eta, 1+eta, size=N_MC)\n",
    "        a_op.iloc[:, t] = (a_star_t * noise).astype(float)\n",
    "        delta_S_t = delta_S.iloc[:, t].values\n",
    "        Pi_op.iloc[:, t] = (gamma * (Pi_op.iloc[:, t+1] - a_op.iloc[:, t] * delta_S_t)).astype(float)\n",
    "        reward_term = (gamma * a_op.iloc[:, t] * delta_S_t).astype(float)\n",
    "        risk_penalty = risk_lambda * np.var(Pi_op.iloc[:, t])\n",
    "        R_op.iloc[:, t] = (reward_term - risk_penalty).astype(float)\n",
    "\n",
    "    # 12) override on‑policy with off‑policy\n",
    "    a = a_op.copy()\n",
    "    Pi = Pi_op.copy()\n",
    "    R  = R_op.copy()\n",
    "\n",
    "    # 13) build Psi_mat & S_t_mat\n",
    "    num_MC, num_TS = a.shape\n",
    "    a_1_1 = a.values.reshape((1, num_MC, num_TS))\n",
    "    a_1_2 = 0.5 * a_1_1**2\n",
    "    ones_3d = np.ones((1, num_MC, num_TS))\n",
    "    A_stack = np.vstack((ones_3d, a_1_1, a_1_2))\n",
    "    data_mat_swap_idx = np.swapaxes(data_mat_t, 0, 2)\n",
    "    A_2 = np.expand_dims(A_stack, axis=1)\n",
    "    D_2 = np.expand_dims(data_mat_swap_idx, axis=0)\n",
    "    Psi_mat = np.multiply(A_2, D_2).reshape(-1, N_MC, num_TS, order='F')\n",
    "    num_Qbasis = Psi_mat.shape[0]\n",
    "    S_t_mat = np.zeros((num_Qbasis, num_Qbasis, num_TS))\n",
    "    for t in range(num_TS):\n",
    "        P = Psi_mat[:, :, t]\n",
    "        S_t_mat[:, :, t] = P.dot(P.T)\n",
    "\n",
    "    # 14) initialize off‑policy Q\n",
    "    # implied Q-function by input data (using the first form in Eq.(68))\n",
    "    Q_RL = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
    "    Q_RL.iloc[:,-1] = - Pi.iloc[:,-1] - risk_lambda * np.var(Pi.iloc[:,-1])\n",
    "\n",
    "    # optimal Q-function with optimal action\n",
    "    Q_star = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))\n",
    "    Q_star.iloc[:,-1] = Q_RL.iloc[:,-1]\n",
    "\n",
    "    # max_Q_star_next = Q_star.iloc[:,-1].values \n",
    "    max_Q_star = np.zeros((N_MC,T+1))\n",
    "    max_Q_star[:,-1] = Q_RL.iloc[:,-1].values\n",
    "\n",
    "    # 15) The backward loop\n",
    "    for t in range(T-1, -1, -1):\n",
    "        \n",
    "        # calculate vector W_t\n",
    "        S_mat_reg = function_S_vec(t,S_t_mat,reg_param) \n",
    "        M_t = function_M_vec(t,Q_star, R, Psi_mat[:,:,t], gamma)\n",
    "        W_t = np.dot(np.linalg.inv(S_mat_reg), M_t)  \n",
    "        \n",
    "        # reshape to a matrix W_mat  \n",
    "        W_mat = W_t.reshape((3, n_basis), order='F')  \n",
    "            \n",
    "        # make matrix Phi_mat\n",
    "        Phi_mat = data_mat_t[t,:,:].T  \n",
    "\n",
    "        # compute matrix U_mat of dimension N_MC x 3 \n",
    "        U_mat = np.dot(W_mat, Phi_mat)\n",
    "        \n",
    "        # compute vectors U_W^0,U_W^1,U_W^2 as rows of matrix U_mat  \n",
    "        U_W_0 = U_mat[0,:]\n",
    "        U_W_1 = U_mat[1,:]\n",
    "        U_W_2 = U_mat[2,:]\n",
    "        \n",
    "        # use hedges 'a_dp' computed as in DP approach:\n",
    "        # in this way, errors of function approximation do not back-propagate. \n",
    "        # This provides a stable solution, \n",
    "        \n",
    "        max_Q_star[:,t] = U_W_0 + a_dp.loc[:,t] * U_W_1 + 0.5 * (a_dp.loc[:,t]**2) * U_W_2       \n",
    "      \n",
    "        # update dataframes     \n",
    "        Q_star.loc[:,t] = max_Q_star[:,t]\n",
    "      \n",
    "        # update the Q_RL solution given by a dot product of two matrices W_t Psi_t\n",
    "        Psi_t = Psi_mat[:,:,t].T \n",
    "        Q_RL.loc[:,t] = np.dot(Psi_t, W_t)\n",
    "\n",
    "    off_policy_price = abs(Q_RL.iloc[:, 0].mean())\n",
    "\n",
    "    # 16) Black–Scholes\n",
    "    def bs_price_call():\n",
    "        d1 = (np.log(S0/K)+(r+0.5*sigma**2)*M)/(sigma*np.sqrt(M))\n",
    "        d2 = d1 - sigma*np.sqrt(M)\n",
    "        return S0*norm.cdf(d1)-K*np.exp(-r*M)*norm.cdf(d2)\n",
    "    def bs_price_put():\n",
    "        d1 = (np.log(S0/K)+(r+0.5*sigma**2)*M)/(sigma*np.sqrt(M))\n",
    "        d2 = d1 - sigma*np.sqrt(M)\n",
    "        return K*np.exp(-r*M)*norm.cdf(-d2)-S0*norm.cdf(-d1)\n",
    "\n",
    "    # 17) print off‑policy results\n",
    "    print(f\"✅ Off-Policy Price      = {off_policy_price:.2f}\")\n",
    "    print(f\"\\n Black–Scholes = \"\n",
    "          f\"{bs_price_call() if option_type=='call' else bs_price_put():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebd038e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "  • For 3 months (~63 trading days), enter 0.25\n",
      "  • For 6 weeks (~31 trading days), enter 0.115\n",
      "  • For 9 months (~189 trading days), enter 0.75\n",
      "  • For 1.5 years (~378 trading days), enter 1.5\n",
      "  • For 2 years (~504 trading days), enter 2\n",
      "\n",
      "  ↳ You selected 1 year(s) + 0.0 months (~252 trading days).\n",
      "  S₀ = 100.0, μ = 0.05, σ = 0.15, r = 0.03, K = 120.0, M = 1.0 years, Type = PUT\n",
      "\n",
      "\n",
      "🚀 Running HedgeGPT Option Pricing Engine\n",
      "  • Risk-aversion (λ) = 0.001\n",
      "  • Methods: On-Policy DP and Off-Policy RL\n",
      "  • Benchmark: Black–Scholes Formula\n",
      "\n",
      "✅ On-Policy Price       = 18.30\n",
      "✅ Off-Policy Price      = 18.32\n",
      "\n",
      " Black–Scholes = 17.76\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
